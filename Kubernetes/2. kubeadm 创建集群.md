Kubernetes 官方提供的一个用于快速搭建和管理 Kubernetes 集群的工具。它提供了简化的流程和命令，用于初始化 Kubernetes 集群，并负责安装和配置集群的核心组件

主要用途就是

1. 初始化集群
2. 加入工作节点或主节点

## 准备环境

确保集群中的所有机器的网络彼此均能相互连接，同时开启机器上的某些端口。请参见[这里](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports)

节点之中不可以有重复的主机名，可以考虑手动修改主机名

```bash
sudo hostnamectl set-hostname 新主机名
```

交换分区禁用，kubelet 的默认在节点上检测到交换内存时无法启动，参阅[交换内存管理](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/#swap-memory)

```bash
# 将暂时禁用交换分区
sudo swapoff -a

# 永久禁用，修改 `/etc/fstab` 文件 或者 `systemd.swap` 文件
sudo nano /etc/fstab
# 查找包含 `swap` 的行，删除或注释掉这些行（在行首加 `#`）
# 保存退出，重启机器查看是否禁用成功 free -h
```

安装容器运行时，负责运行容器的软件如 Docker，这里不过多介绍。安装完成容器运行时后还需要单独安装 **CRI**，常见的容器运行时接口有，具体请参阅 [CRI 版本支持](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-versions)

- [containerd](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd)
- [CRI-O](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cri-o)
- [Docker Engine](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker)
- [Mirantis Container Runtime](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#mcr)

因为有很多的容器运行时，所以 K8s 为了支持这些不同的容器运行时，就需要对 K8s 大量的修改，这个时候，就有了 CRI，用来提供一种接口标准，这样 K8s 就不再依赖特定容器，使得不同的容器运行时可以通过实现这些 API 来与 Kubernetes 兼容

> 自 1.24 版起，Dockershim 已从 Kubernetes 项目中移除。阅读 [Dockershim 移除的常见问题](https://kubernetes.io/zh-cn/dockershim)了解更多详情。

这里以 `docker` 为例，就需要再安装一个 `cri-dockerd` , 对于 `cri-dockerd`，默认情况下，CRI 套接字是 `/run/cri-dockerd.sock`

查看自己系统的架构相关信息

```bash
lsb_release -rc
dpkg --print-architecture
```

然后再  选择合适的版本[cri-dockerd](https://github.com/Mirantis/cri-dockerd/releases)，下载的到服务器，没有就选相同系统最新的那个吧

```bash
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.15/cri-dockerd_0.3.15.3-0.ubuntu-jammy_amd64.deb
# 可以考虑使用镜像 在源网址前加上 `https://hub.gitmirror.com/`，例如：
wget https://hub.gitmirror.com/https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.15/cri-dockerd_0.3.15.3-0.ubuntu-jammy_amd64.deb
```

安装 `cri-dockerd`clear

```bash
dpkg -i cri-dockerd_0.3.15.3-0.ubuntu-jammy_amd64.deb
```

查看 `cri-dockerd` 状态

```bash
systemctl status cri-docker
```

这样就安装完成了

## 安装 kubeadm、kubelet 和 kubectl

kubeadm **不能**帮你安装或者管理 `kubelet` 或 `kubectl`，所以需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配，如果网络很慢可以考虑使用镜像 [阿里云](https://developer.aliyun.com/mirror/kubernetes)

更新 `apt` 包索引并安装使用 Kubernetes `apt` 仓库所需要的包

```shell
sudo apt-get update
# apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```

下载用于 Kubernetes 软件包仓库的公共签名密钥

```shell
# 如果 `/etc/apt/keyrings` 目录不存在，则应在 curl 命令之前创建它，请阅读下面的注释。
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# 阿里云
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

添加 Kubernetes `apt` 仓库，此仓库仅包含适用于 Kubernetes 1.31 的软件包； 对于其他 Kubernetes 次要版本，则需要更改 URL 中的 Kubernetes 次要版本以匹配你所需的次要版本

```shell
# 此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置。
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# 阿里云
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

更新 `apt` 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本

```shell
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环

配置 cgroup 驱动，在版本 1.22 及更高版本中，如果用户没有在 `KubeletConfiguration` 中设置 `cgroupDriver` 字段， `kubeadm` 会将它设置为默认值 `systemd`，具体可见 [cgroupfs驱动](https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver)


## 创建集群

将主节点的 `ip` 设置一下 host

```bash
echo "192.168.10.34 cluster-endpoint" >> /etc/hosts
# 或者手动编辑
nano /etc/hosts
```

再主节点上运行  `init` 来创建集群

```bash
kubeadm init \
--apiserver-advertise-address=192.168.10.34 \
--control-plane-endpoint=cluster-endpoint \
--kubernetes-version v1.31.0 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=172.100.0.0/16 \
--cri-socket=/var/run/cri-dockerd.sock
```

- apiserver-advertise-address
	- 要监听并对外提供服务的网络接口的 IP 地址，这里设置未主节点的 IP
	- 它确保当其他组件（如 `kubelet`、`kube-proxy`、节点）需要与 API Server 通信时，能够通过这个地址连接到 API Server
- control-plane-endpoint
	- 控制平面的统一访问入口，这里指定为 host 设置的域名，当主节点变化时改host就可以了
	- 使得所有集群节点通过该入口访问 Kubernetes API Server，而不依赖单个主节点的 IP 地址
- kubernetes-version
	- 精确控制要安装的 Kubernetes 版本，确保集群中的所有节点运行一致的版本
- service-cidr
	- 用于指定集群中所有 **Service** 资源的虚拟 IP 地址范围，Service IP 地址在该范围内动态分配
- pod-network-cidr
	- 每个 Pod 都有自己的 IP 地址，通过该地址与集群中的其他 Pod 进行通信
		- 确保不与其他的网络如物理网络或 Service 的虚拟 IP 地址发生冲突
- cri-socket
	- 通过 Unix 套接字路径指定 Kubernetes 使用哪个容器运行时，这里指定为 cri-dockerd

这个时候还不能使用 `kubectl`命令的，还需要运行如下命令，`init` 成功后也会提示如下命令

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

如果你是 `root` 用户，则可以运行：

```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
```

只需要运行提示的 `kubeadm join` 命令就可以加入主节点了，例如加入为工作节点

```bash
kubeadm join cluster-endpoint:6443 --token m355zp.go41dwutu43ie47d --discovery-token-ca-cert-hash sha256:b91f5516424e3c54046584364b1f9e9298f597d7401b927d1942aaf6b127bc1c
```

加入后就可以在主节点使用 `kubectl get node` 查看已经加入的节点
### 安装 Pod 网络附加组件

**必须部署一个基于 Pod 网络插件的[容器网络接口](https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)（CNI）。在安装网络之前，集群 DNS (CoreDNS) 将不会启动**，安装这个主要是让 Pod 可以相互通信， Kubernetes 支持的网络插件有很多例如

- [Calico](https://www.tigera.io/project-calico/)
- [Cilium](https://github.com/cilium/cilium)

这里选着 Calico，更为主流稳定

安装 Tigera Operator，Operator 是一个控制器，它负责在 Kubernetes 集群中管理和自动化 Calico 的生命周期

```bash
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml

# 或者使用镜像
kubectl create -f https://raw.gitmirror.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml
```

配置 Calico 网络，用于创建 Calico 的自定义资源实例，这里因为修改 `init` 的 `cidr` 了 所以得先下载下来这个配置文件修改一致之后才能继续创建

```bash
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml

# 或者使用镜像 
wget https://raw.gitmirror.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml
```

下载下来的文件大致如下 ,我们只需要修改 `cidr` 和 `init` 的时候一致就好，然后就可以保存退出了

```yaml
# Calico 的基本安装配置。
# 更多信息，请参见https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # 配置 Calico 网络
  calicoNetwork:
    ipPools:
    - name: default-ipv4-ippool
      blockSize: 26
      cidr: 172.100.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()

---

# 配置 Calico API 服务器。
# 更多信息，请参见https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
```

然后就可以用这个配置文件创建 Calico 了

```bash
kubectl create -f custom-resources.yaml
```

使用如下命令确保所有的POD 都在运行

```bash
watch kubectl get pods -n calico-system
```

每个 Pod 都运行起来后就代表安装完成了

安装 Pod 网络后，你可以通过在 `kubectl get pods --all-namespaces` 输出中检查 CoreDNS Pod 是否 `Running` 来确认其是否正常运行。 一旦 CoreDNS Pod 启用并运行，就可以加入节点了

如果你的网络无法正常工作或 CoreDNS 不在 `Running` 状态，请查看 `kubeadm` 的[故障排除指南](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)。
## 补充

重置集群 `cri-dockerd` 的方式

```sh
sudo kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock
```

清理相关配置文件

```sh
sudo rm -rf /etc/kubernetes/
sudo rm -rf /var/lib/etcd/
sudo rm -rf ~/.kube/
```

获取新的加入令牌

```bash
kubeadm token create --print-join-command
```
